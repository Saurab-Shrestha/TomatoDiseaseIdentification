{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Pillow\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 7])\n",
      "torch.Size([7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (c1): Conv2D()\n",
       "  (m1): MaxPool()\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (c2): Conv2D()\n",
       "  (m2): MaxPool()\n",
       "  (c3): Conv2D()\n",
       "  (m3): MaxPool()\n",
       "  (c4): Conv2D()\n",
       "  (m4): MaxPool()\n",
       "  (c5): Conv2D()\n",
       "  (m5): MaxPool()\n",
       "  (d1): Dense()\n",
       "  (d2): Dense()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "export_file = 'final_model.pth'\n",
    "\n",
    "from final_model import Conv2D, MaxPool, Dense, CNNModel\n",
    "model = CNNModel()\n",
    "\n",
    "model.load_state_dict(torch.load(export_file,map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from model import CNNModel\n",
    "# from model import Conv2D, MaxPool, Dense\n",
    "# # Load the PyTorch model from a .pkl file\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "    0: 'Bacterial_spot',\n",
    "    1: 'Early_blight',\n",
    "    2: 'Healthy',\n",
    "    3: 'Late_blight',\n",
    "    4: 'Mosiac_virus',\n",
    "    5: 'Septoria_leaf_spot',\n",
    "    6: 'YellowLeaf_curl'\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_disease(img):\n",
    "    image = Image.open(img).convert('RGB')\n",
    "\n",
    "    # Apply transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image)\n",
    "\n",
    "    # Pass through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #image = image # Move input tensor to the GPU\n",
    "        output = model(image.unsqueeze(0))  # Add batch dimension\n",
    "    predicted_probs = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    actual_class = idx_to_class[predicted_class]\n",
    "    confidence = predicted_probs[0][predicted_class].item()\n",
    "\n",
    "    # Print the predicted class and confidence\n",
    "    print('Predicted class:', actual_class)\n",
    "    print('Confidence:', confidence)\n",
    "\n",
    "    return actual_class,confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Early_blight\n",
      "Confidence: 0.9951351284980774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Early_blight', 0.9951351284980774)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = \"C:/Users/Saurab/Desktop/Tomato Leaf disease detection/early_blight21.jpg\"\n",
    "\n",
    "predict_disease(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 14315\n",
      "Image shape: torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:/Users/Saurab/Desktop/Plant Disease Final/Dataset/Plant\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# Get the first batch of data\n",
    "inputs, labels = next(iter(dataloader))\n",
    "\n",
    "# Get the shape of the images\n",
    "image_shape = inputs[0].shape\n",
    "\n",
    "# Print the number of images and their shape\n",
    "print(f\"Number of images: {len(dataloader.dataset)}\")\n",
    "print(f\"Image shape: {image_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train size :9018\n",
      "length of validation size :3865\n",
      "length of test size :5297\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(0.90 * len(dataset)))  # train_size\n",
    "validation = int(np.floor(0.70 * split))   # validation\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "print(f\"length of train size :{validation}\")\n",
    "print(f\"length of validation size :{split - validation}\")\n",
    "print(f\"length of test size :{len(dataset)-validation}\")\n",
    "\n",
    "train_indices, validation_indices, test_indices = (\n",
    "    indices[:validation],\n",
    "    indices[validation:split],\n",
    "    indices[split:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train size :9018\n",
      "length of validation size :3865\n",
      "length of test size :5297\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(0.90 * len(dataset)))  # train_size\n",
    "validation = int(np.floor(0.70 * split))   # validation\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "print(f\"length of train size :{validation}\")\n",
    "print(f\"length of validation size :{split - validation}\")\n",
    "print(f\"length of test size :{len(dataset)-validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, sampler=test_sampler\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, sampler=validation_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.980261698824573\n",
      "Test Accuracy : 0.9776536312849162\n",
      "Validation Accuracy : 0.9793014230271668\n"
     ]
    }
   ],
   "source": [
    "def accuracy(loader):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        n_correct += (predictions == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "    acc = n_correct / n_total\n",
    "    return acc\n",
    "\n",
    "train_acc = accuracy(train_loader)\n",
    "test_acc = accuracy(test_loader)\n",
    "validation_acc = accuracy(validation_loader)\n",
    "\n",
    "print(\n",
    "    f\"Train Accuracy : {train_acc}\\nTest Accuracy : {test_acc}\\nValidation Accuracy : {validation_acc}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55ee6eaf08ae5ebc13ed428a7893765bfe6ee53bba406d18f1cefdbb97427938"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
